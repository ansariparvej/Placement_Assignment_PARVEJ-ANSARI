{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c8c1a51",
   "metadata": {},
   "source": [
    "## Question_1_Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16802449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries:\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3c484d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37173/1324957056.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('data_to_utilise/chromedriver')\n"
     ]
    }
   ],
   "source": [
    "# Web driver setup:\n",
    "driver = webdriver.Chrome('data_to_utilise/chromedriver')\n",
    "# Youtube video to get comments:\n",
    "video_url = 'https://www.youtube.com/watch?v=NWzbdWf7Yts&ab_channel=KrishNaik'  \n",
    "driver.get(video_url)\n",
    "\n",
    "scrolls = 3\n",
    "for _ in range(scrolls):\n",
    "    driver.execute_script('window.scrollTo(0, document.documentElement.scrollHeight);')\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "10412d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Check out the Data Analyst RoadMap: https://www.youtube.com/watch?v=xSCeY4dg17Y', '@Krish Naik - Hi Krish, I listen to your courses seriously and absorb them as much as I can. My genuine respect for you.  Requesting Krish, to create playlists on FAST API - ML  if possible and full playlists for other areas like you did it for ML and deep learning playlists.', 'Hello Krish , How are you ? \\n\\n I am new in ML , but also I am in my mid-career level.. having 15+ years of IT experience.. I have been working in D, ETL areas, but need to switch to ML. Can you please suggest me what are the key things I can learn at this stage and if you could suggest me any suitable learning path ( like any Youtube video link/ course link) to start with, will be really helpful. Thank you !!', '@krish Naik - Hi Krish, I am told by my friend to watch your vids for good understanding of Data Science role. I’m from a finance sector and want to switch to data science. So this video kind of helps me to go through your playlists accordingly. I have one important question, i.e. I also want to enroll to Data science courses where a certification has its value as I’m from finance sector so a validated certification would help me. So which is the best one to enroll in regards to materials/concepts for this year and also placement support. Thanks. Hoping for a reply.', 'Always a pleasure listening to you \\n\\n\\nGreat work', 'sir i wanted to ask if we should prefer watching 7 days live videos or the particular playlist', 'A) How does this course help for an existing student of ineuron in DS?  B)What will be the extra curriculum added.  C)  will the project part is LIVE?', 'Just a question, when should I go for the live lectures and when to the offline uploaded ones(those which are 100+ videos playlist).\\n\\nThanks in Advance', 'Heyy krish i have a question for you can i develop a career with a mixture of data science + devOps?', 'Hi Krish, can we have a Machine Learning Engineer Roadmap in the same manner? Thanks.', \"Hey Krish, can you please make a video on revised best online courses (which provides good material as well as a valuable certification) ?? There's a video but, it's from 3 years back. Thank you in advance.\", '@krish Naik - How important is it for someone transitioning into data science to have a certification or it is ok if you self-study this skillset', 'Sir, please explain the difference between the iNeuron \"Full stack data science 2.0\" course and the PWskills \"Data science masters\" course offered by you, which one to buy???', 'All content of pwskills should be same as INeuron .... It means it is a way to explore business... I will prefer ineuron course... If possible kindly increase discount from 50%', 'thankyou krish you r amazing in providing roadmaps', 'Hats off to you sir', 'Thank you so much sir ... I love ur teaching...', 'great start of 2023 after seeing your detailed roadmap for DS', 'Hii krish love your videos following regularly... Pls guide me.. I am working NON IT. I am. Planning to do DE and DA ML... Some people are told me.. Only IT background experience people can enter DS.. They guide me should focus first full stack then DS.. is this true or with out othe domain exp.. Can i get DATA related jobs.. After learning and best projects\\n. Pls pls guide me', 'Thank you so much for this wonderful guide']\n"
     ]
    }
   ],
   "source": [
    "# Extracting comments from video:\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "comment_items = soup.find_all('yt-formatted-string', class_='style-scope ytd-comment-renderer')\n",
    "\n",
    "comments = []\n",
    "for item in comment_items:\n",
    "    comment = item.text.strip()\n",
    "    comments.append(comment)\n",
    "print(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1dcd91f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments extracted successfully and saved in the CSV file.\n"
     ]
    }
   ],
   "source": [
    "# Storing comments in a CSV file:\n",
    "\n",
    "csv_file_path = 'YT_comments.csv' \n",
    "\n",
    "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['Comment'])\n",
    "\n",
    "    for comment in comments:\n",
    "        csv_writer.writerow([comment])\n",
    "\n",
    "print(\"Comments extracted successfully and saved in the CSV file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4bcc1347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 3 0 0 4 3 0 4 3 4 2 0 0 0 0 1 1 0 3 4 4 2 2 4 0 4 3 0 0 3 1 4 0 1 0 3\n",
      " 4 4 3 0 1 1 3 4 1 4 3 1 1 3 0 3 0 4 1 1 1 4 1 0 1 4 4 3 1 4 4 4 4 4 0 4 4\n",
      " 1 4 2 2 0 3 1 4 4 0 4 4 1 3 4 0 4 0 3 3 1 3 0 3 3 0 4 0 4 4 0 0 2 4 4 1 3\n",
      " 3 2 4 3 0 1 3 0 4 1 4 1 3 0 4 3 1 0 0 3 1 0 4 3 4 4 4 1 1 4 0 0 0 3 4 4 4\n",
      " 4 3 2 0 0 2 2 1 4 4 3 2 4 4 4 4 0 0 3 4 4 1 2 1 4 3 0 4 0 4 4 1 4 4 0 4 0\n",
      " 0 0 2 3 1 4 2 2 4 4 2]\n",
      "3\n",
      "The most demanding topic is Topic 4: krish science\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Extract Most Demanding topic name:\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Preprocessing\n",
    "nltk.download('stopwords')\n",
    "stop_words = list(nltk.corpus.stopwords.words('english'))  # Convert stopwords to a list\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Fit and transform the comment data\n",
    "comment_vectors = vectorizer.fit_transform(comments)\n",
    "\n",
    "# Modeling using Latent Dirichlet Allocation (LDA)\n",
    "n_topics = 5  # Set the desired number of topics\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda_model.fit(comment_vectors)\n",
    "\n",
    "# Demanding topic\n",
    "dominant_topic = lda_model.components_.argmax(axis=0)\n",
    "print(dominant_topic)\n",
    "most_demanding_topic = dominant_topic[0]\n",
    "print(most_demanding_topic)\n",
    "\n",
    "print(f\"The most demanding topic is Topic {most_demanding_topic + 1}: {' '.join(top_words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96725dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
