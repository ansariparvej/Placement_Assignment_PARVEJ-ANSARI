{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c8c1a51",
   "metadata": {},
   "source": [
    "## Question_3_Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16802449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a870215",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Load CSV file:\n",
    "\n",
    "csv_file = 'Question_2_Solution.csv'\n",
    "\n",
    "# Saving text in a Variable:\n",
    "\n",
    "text = \"\"\n",
    "with open(csv_file, 'r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        text += ' '.join(row) + '\\n'\n",
    "\n",
    "# Print the text:\n",
    "#print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b9c47d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.5.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m546.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ali/anaconda3/lib/python3.9/site-packages (from spacy) (4.65.0)\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m327.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m481.9 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/ali/anaconda3/lib/python3.9/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ali/anaconda3/lib/python3.9/site-packages (from spacy) (1.23.5)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (491 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.6/491.6 kB\u001b[0m \u001b[31m530.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ali/anaconda3/lib/python3.9/site-packages (from spacy) (2.29.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Requirement already satisfied: setuptools in /home/ali/anaconda3/lib/python3.9/site-packages (from spacy) (66.0.0)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m97.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m162.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.10-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (924 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m924.9/924.9 kB\u001b[0m \u001b[31m220.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: jinja2 in /home/ali/anaconda3/lib/python3.9/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/ali/anaconda3/lib/python3.9/site-packages (from spacy) (1.9.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ali/anaconda3/lib/python3.9/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ali/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ali/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ali/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ali/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ali/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m534.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /home/ali/anaconda3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ali/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, murmurhash, langcodes, catalogue, blis, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 spacy-3.5.3 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.10 typer-0.7.0 wasabi-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c484d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For extracting the keywords\n",
    "\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def get_hotwords(text):\n",
    "    result = []\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN'] \n",
    "    doc = nlp(text.lower()) \n",
    "    for token in doc:\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag):\n",
    "            result.append(token.text)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a9b6c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "york\n",
      "abdulghani\n",
      "banks\n",
      "payment\n",
      "nurul\n",
      "vulnerabilities\n",
      "oct\n",
      "models\n",
      "single\n",
      "color\n"
     ]
    }
   ],
   "source": [
    "# Printing Extracted Keywords:\n",
    "output = set(get_hotwords(text))\n",
    "#print(output)\n",
    "most_common_list = Counter(output).most_common(10)\n",
    "for item in most_common_list:\n",
    "  print(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e804443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: nurul, single, abdulghani, banks, payment\n",
      "Topic 2: color, vulnerabilities, abdulghani, nurul, payment\n",
      "Topic 3: york, abdulghani, single, nurul, banks\n",
      "Topic 4: models, single, banks, abdulghani, payment\n",
      "Topic 5: oct, payment, banks, abdulghani, single\n"
     ]
    }
   ],
   "source": [
    "key_words = ['york', 'abdulghani' ,'banks' ,'payment', 'nurul', 'vulnerabilities', 'oct', 'models', 'single', 'color']\n",
    "\n",
    "# Convert keywords into a document-term matrix\n",
    "vectorizer = CountVectorizer()\n",
    "dtm = vectorizer.fit_transform(key_words)\n",
    "\n",
    "# Choose the number of topics\n",
    "num_topics = 5\n",
    "\n",
    "# Apply Latent Dirichlet Allocation (LDA)\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Get the top keywords for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "top_keywords = []\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_indices = topic.argsort()[-5:][::-1]  # Get top 5 keywords per topic\n",
    "    keywords = [feature_names[idx] for idx in top_indices]\n",
    "    top_keywords.append(', '.join(keywords))\n",
    "\n",
    "# Print the topics and associated top keywords\n",
    "for topic_idx, keywords in enumerate(top_keywords):\n",
    "    print(f'Topic {topic_idx + 1}: {keywords}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604bfae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
